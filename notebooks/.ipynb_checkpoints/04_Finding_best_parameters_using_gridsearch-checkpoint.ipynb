{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b467b0c6",
   "metadata": {},
   "source": [
    "# Finding best parameters using GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0035b8b1",
   "metadata": {},
   "source": [
    "## Navigation\n",
    "<ul>\n",
    "<li><a href=\"#grid\">Grid Search to find best parameter</a></li>\n",
    "<li><a href=\"#lr\">Logistic Regression</a></li>\n",
    "<li><a href=\"#knn\">K-Nearest Neighbors</a></li>\n",
    "<li><a href=\"#rf\">Random Forest</a></li>\n",
    "<li><a href=\"#xgb\">XGBoost</a></li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "222c9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imblearn.over_sampling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34ee858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X = df.drop(\"LOAN_DEFAULT\" , axis = 1)\n",
    "y = df[\"LOAN_DEFAULT\"]\n",
    "X_train , X_val , y_train , y_val = train_test_split(X, y, test_size = 0.2, random_state=7)\n",
    "# X_train_sub_set , X_vald_sub_set , y_train_sub_set , y_vald_sub_set = train_test_split(x[0:1000], y[0:1000] , test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ed943fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdulium/miniconda3/envs/t5/lib/python3.7/site-packages/imblearn/utils/_validation.py:300: UserWarning: After over-sampling, the number of samples (95884) in class 1 will be larger than the number of samples in the majority class (class #0 -> 84782)\n",
      "  f\"After over-sampling, the number of samples ({n_samples})\"\n"
     ]
    }
   ],
   "source": [
    "####### SUB #########\n",
    "X_train_sub , X_val_sub , y_train_sub , y_val_sub = train_test_split(X, y, test_size = 0.5, random_state=7)\n",
    "\n",
    "# ROS ratio argument\n",
    "n_pos_train = np.sum(y_train_sub == 1)\n",
    "n_neg_train = np.sum(y_train_sub == 0)\n",
    "\n",
    "ratio = {1 : n_pos_train * 4, 0 : n_neg_train} \n",
    "\n",
    "\n",
    "# randomly oversample positive samples 4 times\n",
    "\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy=ratio, random_state = 10)\n",
    "    \n",
    "X_tr_smote, y_tr_smote = smote.fit_resample(X_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1238d7e",
   "metadata": {},
   "source": [
    "## Scale data using Z score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "676dd0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling data\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_val_scaled = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43710565",
   "metadata": {},
   "source": [
    "# GridSearch finding best parameters\n",
    "<a id='grid'></a>\n",
    "<a href=\"#\">Back to top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a13e8",
   "metadata": {},
   "source": [
    "> \"SUB\" tags means that the execution of GridSearchCV on this part has taken subset of data and not all based on 0.5 split to speed the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07322d",
   "metadata": {},
   "source": [
    "## Logistics Regression\n",
    "<a id='lr'></a>\n",
    "<a href=\"#\">Back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using F1 score\n",
    "param_grid = {\n",
    "      'C' : np.linspace(0.01,50,100),\n",
    "    'solver': ['lbfgs', 'saga'],\n",
    "      'penalty' : ['l1', 'l2'],\n",
    "    \n",
    "}\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(ss.fit_transform(X_train_scaled), y_train)\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, scoring='f1', n_jobs=-1)\n",
    "grid.fit(X_train , y_train)\n",
    "# view the complete results\n",
    "# print(\"grid_cv\" , grid.cv_results_)\n",
    "# examine the best model\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "\n",
    "# using accuracy\n",
    "param_grid = {\n",
    "      'C' : np.linspace(0.01,50,100),\n",
    "    'solver': ['lbfgs', 'saga'],\n",
    "      'penalty' : ['l1', 'l2'],\n",
    "    \n",
    "}\n",
    "\n",
    "model = LogisticRegression(n_jobs=-1)\n",
    "model.fit(ss.fit_transform(X_train), y_train)\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_train , y_train)\n",
    "## view the complete results\n",
    "## print(\"grid_cv\" , grid.cv_results_)\n",
    "## examine the best model\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)\n",
    "print(\"Best score: \", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "385c0289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdulium/miniconda3/envs/t5/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "200 fits failed out of a total of 800.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/abdulium/miniconda3/envs/t5/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/abdulium/miniconda3/envs/t5/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/abdulium/miniconda3/envs/t5/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
      "    % (solver, penalty)\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/abdulium/miniconda3/envs/t5/lib/python3.7/site-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173        nan 0.53767173\n",
      " 0.54704814 0.53767173        nan 0.53767173 0.54704814 0.53767173\n",
      "        nan 0.53767173 0.54704814 0.53767173]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best estimator:  LogisticRegression(C=0.01, n_jobs=-1)\n",
      "Best score:  0.5470481440890926\n"
     ]
    }
   ],
   "source": [
    "################### SUB ###################\n",
    "param_grid = {\n",
    "      'C' : np.linspace(0.01,50,100),\n",
    "    'solver': ['lbfgs', 'saga'],\n",
    "      'penalty' : ['l1', 'l2'],\n",
    "    \n",
    "}\n",
    "\n",
    "model = LogisticRegression(n_jobs=-1)\n",
    "model.fit(ss.fit_transform(X_tr_smote), y_tr_smote)\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, scoring='accuracy', cv=2, n_jobs=-1)\n",
    "grid.fit(X_tr_smote , y_tr_smote)\n",
    "## view the complete results\n",
    "## print(\"grid_cv\" , grid.cv_results_)\n",
    "## examine the best model\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)\n",
    "print(\"Best score: \", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b028a17",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor\n",
    "<a id='knn'></a>\n",
    "<a href=\"#\">Back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d024fb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 92 candidates, totalling 460 fits\n",
      "Best params:  {'metric': 'euclidean', 'n_neighbors': 24, 'weights': 'uniform'}\n",
      "Best estimator:  KNeighborsClassifier(metric='euclidean', n_neighbors=24)\n",
      "Best score:  0.7773039734622502\n",
      "3961.502706050873\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# define param_grid\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(3,26)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# grid search\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, verbose =1, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fd2c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 92 candidates, totalling 184 fits\n",
      "Best params:  {'metric': 'manhattan', 'n_neighbors': 22, 'weights': 'distance'}\n",
      "Best estimator:  KNeighborsClassifier(metric='manhattan', n_neighbors=22, weights='distance')\n",
      "Best score:  0.6597422868719073\n",
      "51.98113213380178\n"
     ]
    }
   ],
   "source": [
    "################### SUB ###################\n",
    "start = time.time()\n",
    "\n",
    "# define param_grid\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(3,26)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# grid search\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=2, verbose =1, n_jobs=-1)\n",
    "\n",
    "grid.fit(ss.fit_transform(X_tr_smote), y_tr_smote)\n",
    "\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a2aaf",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "<a id='rf'></a>\n",
    "<a href=\"#\">Back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7a80706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best params:  {'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Best estimator:  RandomForestClassifier(max_depth=15, min_samples_leaf=2, min_samples_split=10,\n",
      "                       random_state=7)\n",
      "Best score:  0.7791774911012929\n",
      "33.56065210898717\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# define param_grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 300, 500],\n",
    "    \"max_depth\": [5, 15, 25],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [2, 5, 10]\n",
    "\n",
    "}\n",
    "\n",
    "# grid search\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=7), param_grid, verbose =1, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "799b006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 180 candidates, totalling 360 fits\n",
      "Best params:  {'max_depth': 25, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 250}\n",
      "Best estimator:  RandomForestClassifier(max_depth=25, min_samples_leaf=2, n_estimators=250,\n",
      "                       random_state=7)\n",
      "Best score:  0.6726113380492178\n",
      "11.624450532595317\n"
     ]
    }
   ],
   "source": [
    "################### SUB ###################\n",
    "start = time.time()\n",
    "\n",
    "# define param_grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": list(range(100,300,50)),\n",
    "    \"max_depth\": list(range(5,30,5)),\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [2, 5, 10]\n",
    "\n",
    "}\n",
    "\n",
    "# grid search\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=7), param_grid, cv=2, verbose =1, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_tr_smote, y_tr_smote)\n",
    "\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86059444",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "<a id='xgb'></a>\n",
    "<a href=\"#\">Back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6f0ab5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdulium/miniconda3/envs/t5/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:55:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best params:  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'objective': 'binary:logistic'}\n",
      "Best estimator:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
      "              max_depth=5, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=7,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "Best score:  0.7788039359748878\n",
      "261.58992855151496\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 300, 500],\n",
    "    \"max_depth\": [5, 15, 25],\n",
    "    \"learning_rate\": np.linspace(0.1, 1, 15),\n",
    "    \"objective\":[\"binary:logistic\"]\n",
    "\n",
    "}\n",
    "\n",
    "# grid search\n",
    "grid = GridSearchCV(XGBClassifier(random_state=7), param_grid, verbose =1, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8918cd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 60 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdulium/miniconda3/envs/t5/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:53:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best params:  {'learning_rate': 0.325, 'max_depth': 25, 'n_estimators': 250}\n",
      "Best estimator:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.325, max_delta_step=0,\n",
      "              max_depth=25, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=250, n_jobs=16,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=7,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "Best score:  0.7191502551669933\n",
      "26.475709172089896\n"
     ]
    }
   ],
   "source": [
    "################### SUB ###################\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": list(range(100,300,50)),\n",
    "    \"max_depth\": [5, 15, 25],\n",
    "    \"learning_rate\": np.linspace(0.1, 1, 5)\n",
    "\n",
    "}\n",
    "\n",
    "# grid search\n",
    "grid = GridSearchCV(XGBClassifier(random_state=7), param_grid, cv=2, verbose =1, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_tr_smote, y_tr_smote)\n",
    "\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)\n",
    "print(\"Best score: \", grid.best_score_)\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5",
   "language": "python",
   "name": "t5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
